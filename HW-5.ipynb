{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tokenizers\n",
      "  Downloading tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tokenizers\n",
      "Successfully installed tokenizers-0.9.4\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 20.3.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import lil_matrix\n",
    "from collections import Counter, defaultdict\n",
    "from tokenizers import CharBPETokenizer, Tokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(corpus: str) -> dict:\n",
    "    \"\"\"Step 1. Build vocab from text corpus\"\"\"\n",
    "\n",
    "    # Separate each char in word by space and add mark end of token\n",
    "    tokens = [\" \".join(word) for word in corpus.split()]\n",
    "    \n",
    "    # Count frequency of tokens in corpus\n",
    "    vocab = Counter(tokens)  \n",
    "\n",
    "    return vocab\n",
    "\n",
    "def get_stats(vocab: dict) -> dict:\n",
    "    \"\"\"Step 2. Get counts of pairs of consecutive symbols\"\"\"\n",
    "\n",
    "    pairs = defaultdict(int)\n",
    "    for word, frequency in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        # Counting up occurrences of pairs\n",
    "        for i in range(len(symbols) - 1):\n",
    "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
    "\n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair: tuple, v_in: dict) -> dict:\n",
    "    \"\"\"Step 3. Merge all occurrences of the most frequent pair\"\"\"\n",
    "    \n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        # replace most frequent pair in all vocabulary\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "\n",
    "    return v_out\n",
    "\n",
    "def get_tokens_from_vocab(vocab):\n",
    "    tokens_frequencies = defaultdict(int)\n",
    "    vocab_tokenization = {}\n",
    "    for word, freq in vocab.items():\n",
    "        word_tokens = word.split()\n",
    "        for token in word_tokens:\n",
    "            tokens_frequencies[token] += freq\n",
    "        vocab_tokenization[''.join(word_tokens)] = word_tokens\n",
    "    return tokens_frequencies, vocab_tokenization\n",
    "\n",
    "def tokenize_text(string, sorted_tokens, unknown_token='</u>'):    \n",
    "    if string == '':\n",
    "        return []\n",
    "    if sorted_tokens == []:\n",
    "        return [unknown_token]\n",
    "    string_tokens = []\n",
    "    for i in range(len(sorted_tokens)):\n",
    "        token = sorted_tokens[i]\n",
    "        token_reg = re.escape(token.replace('.', '[.]'))\n",
    "\n",
    "        matched_positions = [(m.start(0), m.end(0)) for m in re.finditer(token_reg, string)]\n",
    "        if len(matched_positions) == 0:\n",
    "            continue\n",
    "        substring_end_positions = [matched_position[0] for matched_position in matched_positions]\n",
    "\n",
    "        substring_start_position = 0\n",
    "        for substring_end_position in substring_end_positions:\n",
    "            substring = string[substring_start_position:substring_end_position]\n",
    "            string_tokens += tokenize_text(string=substring, sorted_tokens=sorted_tokens[i + 1:], unknown_token=unknown_token)\n",
    "            string_tokens += [token]\n",
    "            substring_start_position = substring_end_position + len(token)\n",
    "        remaining_substring = string[substring_start_position:]\n",
    "        string_tokens += tokenize_text(string=remaining_substring, sorted_tokens=sorted_tokens[i + 1:], unknown_token=unknown_token)\n",
    "        break\n",
    "    return string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены до BPE\n",
      "Токены: dict_keys(['б', 'о', 'и', 'у', 'с', 'п', 'ц', 'к', 'н', 'а', 'д', 'р', 'е', 'з', 'ч', 'л', 'ь', 'т', 'м', 'г', 'в', '.', 'я', ',', 'ш', 'й', 'ю', 'ж', 'ы', '1', '4', 'х', 'щ', '«', '»', '6', '9', '(', '2', '-', ')', 'э', 'ф', '—', 'd', 'a', 's', 'i', 't', 'n', 'e', 'r', 'o', 'f', '!', 'ъ', '7', '5', '3', '№', '…', '–', 'l', 'y', 'm', '8', '0', '\"', ':', 'g', 'z', 'u', '$', '%', 'b', 'k', 'w', 'x', 'p', 'h', 'v', \"'\", 'j', 'ё', '/', 'c', ';', '=', '&', '?', 'q', '*', '@', '+', '’', '·', '“', '”', '•', '>', '_', '|', '[', ']', '\\xad', '~', '<', '#', '£', 'ї', 'і', '`'])\n",
      "Количество токенов: 112\n"
     ]
    }
   ],
   "source": [
    "with open('lenta.txt', 'r', encoding='utf-8') as fhand:\n",
    "    corpus = fhand.read()\n",
    "    corpus = corpus.lower()\n",
    "\n",
    "vocab = build_vocab(corpus)  # Step 1\n",
    "\n",
    "print('Токены до BPE')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "print('Токены: {}'.format(tokens_frequencies.keys()))\n",
    "print('Количество токенов: {}'.format(len(tokens_frequencies.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 1 (N=10, K=3)\n",
    "\n",
    "Уже прослеживается появление аффиксов, что, как мне кажется, хорошо. Новость токенизирована так, что ничего осмысленного не выделяется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены после BPE (отсортированные)\n",
      "Токены: ['ени', 'ст', 'ов', 'ра', 'на', 'по', 'ре', 'ко', 'но', 'ро', 'ен', 'то', 'ни', 'го', 'ли', 'ет', 'ер', 'ль', 'ны', 'ка', 'за', 'не', 'ла', 'ри', 'да', 'во', 'та', 'ми', 'те', 'об', 'и', 'с', 'е', 'о', 'в', 'а', 'д', 'м', 'у', 'л', 'т', 'п', 'к', 'я', 'н', 'р', ',', 'й', 'ч', 'б', 'ы', 'з', '.', 'г', 'х', 'ж', 'ь', 'ц', '\"', 'ю', 'ш', 'щ', 'ф', '-', 'э', '0', '1', 'e', '2', 'a', 'o', 's', 'r', '9', 'n', 'i', 't', '5', '3', 'c', '4', 'l', 'ъ', '6', 'm', ')', '(', '8', '7', ':', 'd', 'u', 'p', 'b', 'w', 'f', 'h', 'g', 'k', 'y', 'v', 'x', '%', ';', 'z', 'j', '/', \"'\", '!', 'q', '&', '№', '?', '$', '—', '«', '»', '–', '+', '@', '“', '”', '·', 'ё', '>', '=', '_', '*', '\\xad', '•', '…', '’', '£', '<', 'ї', '|', '[', ']', '~', '#', 'і', '`']\n",
      "Количество токенов: 142\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(corpus)  # Step 1\n",
    "\n",
    "num_merges = 10  # Hyperparameter\n",
    "k = 3\n",
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best_top_k = sorted(pairs, key=pairs.get, reverse=True)[:k]\n",
    "    for best in best_top_k:\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print('Токены после BPE (отсортированные)')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (len(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "print('Токены: {}'.format(sorted_tokens))\n",
    "print('Количество токенов: {}'.format(len(tokens_frequencies.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['г', 'у', 'б', 'ер', 'на', 'то', 'р', 'на', 'з', 'в', 'а', 'л', '1', '2', 'г', 'ла', 'в', 'ны', 'х', 'д', 'о', 'ст', 'и', 'ж', 'ени', 'й', 'ко', 'ст', 'ро', 'м', 'с', 'ко', 'й', 'об', 'ла', 'ст', 'и', 'в', 'э', 'то', 'м', 'го', 'д', 'у', 'п', 'е', 'ре', 'д', 'на', 'ст', 'у', 'п', 'л', 'ени', 'е', 'м', 'н', 'ов', 'о', 'го', 'го', 'да', 'г', 'у', 'б', 'ер', 'на', 'то', 'р', 'с', 'ер', 'г', 'е', 'й', 'с', 'и', 'т', 'ни', 'к', 'ов', 'по', 'д', 'в', 'е', 'л', 'и', 'то', 'г', 'и', 'у', 'х', 'о', 'д', 'я', 'щ', 'е', 'го', 'го', 'да', 'и', 'на', 'з', 'в', 'а', 'л', 'г', 'ла', 'в', 'ны', 'е', 'д', 'о', 'ст', 'и', 'ж', 'ени', 'я', 'ко', 'ст', 'ро', 'м', 'с', 'ко', 'й', 'об', 'ла', 'ст', 'и', ',', 'у', 'з', 'на', 'л', 'k', 'o', 's', 't', 'r', 'o', 'm', 'a', 't', 'o', 'd', 'a', 'y', 'с', 'д', 'е', 'ла', 'л', 'о', 'н', 'э', 'то', 'в', 'и', 'н', 'ст', 'а', 'г', 'ра', 'м', 'е']\n"
     ]
    }
   ],
   "source": [
    "yandex_news = '''\n",
    "Губернатор назвал 12 главных достижений Костромской области в этом году\n",
    "Перед наступлением нового года губернатор Сергей Ситников подвел итоги уходящего года и назвал главные достижения Костромской области, узнал KOSTROMA.TODAY. Сделал он это в Инстаграме.\n",
    "'''\n",
    "print(tokenize_text(string=yandex_news.lower(), sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 2 (N=15, K=5)\n",
    "\n",
    "Количество аффиксов увеличивается. Новость токенизирована так, что ничего осмысленного не выделяется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены после BPE (отсортированные)\n",
      "Токены: ['тель', 'ени', 'про', 'ова', 'ста', 'ско', 'сти', 'что', 'ово', 'ных', 'ски', 'нов', 'ра', 'ст', 'на', 'по', 'но', 'ен', 'ни', 'го', 'ли', 'ет', 'ко', 'ер', 'ов', 'ре', 'пр', 'то', 'ка', 'ро', 'за', 'не', 'ла', 'да', 'об', 'та', 'во', 'ми', 'ль', 'ны', 'ед', 'ва', 'ви', 'со', 'ци', 'ло', 'ся', 'ру', 'ле', 'де', 'си', 'ма', 'ти', 'те', 'от', 'мо', 'ть', 'до', 'ри', 'че', 'ди', 'вы', 'се', 'из', 'па', 'ки', 'бо', 'ин', 'ме', 'ча', 'ля', 'са', 'бы', 'пе', 'сл', 'и', 'е', 'в', 'с', 'у', 'м', 'к', 'о', 'а', 'д', ',', 'н', 'й', 'т', 'я', 'р', 'л', '.', 'г', 'ж', 'з', 'б', 'ы', 'п', 'х', '\"', 'ю', 'ч', 'ш', 'щ', 'ф', 'ь', '-', 'ц', 'э', '0', '1', 'e', '2', 'a', 'o', 's', 'r', '9', 'n', 'i', 't', '5', '3', 'c', '4', 'l', 'ъ', '6', 'm', ')', '(', '8', '7', ':', 'd', 'u', 'p', 'b', 'w', 'f', 'h', 'g', 'k', 'y', 'v', 'x', '%', ';', 'z', 'j', '/', \"'\", '!', 'q', '&', '№', '?', '$', '—', '«', '»', '–', '+', '@', '“', '”', '·', 'ё', '>', '=', '_', '*', '\\xad', '•', '…', '’', '£', '<', 'ї', '|', '[', ']', '~', '#', 'і', '`']\n",
      "Количество токенов: 187\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(corpus)  # Step 1\n",
    "\n",
    "num_merges = 15 # Hyperparameter\n",
    "k = 5\n",
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best_top_k = sorted(pairs, key=pairs.get, reverse=True)[:k]\n",
    "    for best in best_top_k:\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print('Токены после BPE (отсортированные)')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (len(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "print('Токены: {}'.format(sorted_tokens))\n",
    "print('Количество токенов: {}'.format(len(tokens_frequencies.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['г', 'у', 'б', 'ер', 'на', 'то', 'р', 'на', 'з', 'ва', 'л', '1', '2', 'г', 'ла', 'в', 'ных', 'до', 'сти', 'ж', 'ени', 'й', 'ко', 'ст', 'ро', 'м', 'ско', 'й', 'об', 'ла', 'сти', 'в', 'э', 'то', 'м', 'го', 'д', 'у', 'п', 'ер', 'ед', 'на', 'ст', 'у', 'п', 'л', 'ени', 'е', 'м', 'н', 'ово', 'го', 'го', 'да', 'г', 'у', 'б', 'ер', 'на', 'то', 'р', 'с', 'ер', 'г', 'е', 'й', 'си', 'т', 'ни', 'ко', 'в', 'по', 'д', 'в', 'е', 'л', 'и', 'то', 'г', 'и', 'у', 'х', 'о', 'д', 'я', 'щ', 'е', 'го', 'го', 'да', 'и', 'на', 'з', 'ва', 'л', 'г', 'ла', 'в', 'ны', 'е', 'до', 'сти', 'ж', 'ени', 'я', 'ко', 'ст', 'ро', 'м', 'ско', 'й', 'об', 'ла', 'сти', ',', 'у', 'з', 'на', 'л', 'k', 'o', 's', 't', 'r', 'o', 'm', 'a', 't', 'o', 'd', 'a', 'y', 'с', 'де', 'ла', 'л', 'о', 'н', 'э', 'то', 'в', 'ин', 'ста', 'г', 'ра', 'ме']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text(string=yandex_news.lower(), sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперимент 3 (N=20, K=10)\n",
    "\n",
    "Появляются подтокены, которые можно отнести к корням или частям корней, мне кажется, что это нежелательно. Новость токенизирована так, что ничего осмысленного не выделяется"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токены после BPE (отсортированные)\n",
      "Токены: ['россии', 'сообщ', 'ского', 'росси', 'ности', 'ствен', 'ения', 'ного', 'ение', 'пред', 'тель', 'стра', 'кото', 'пере', 'ской', 'ства', 'ется', 'рова', 'ство', 'мини', 'ново', 'ских', 'росс', 'про', 'ста', 'ова', 'что', 'ных', 'пре', 'при', 'ени', 'ной', 'сто', 'сти', 'ции', 'ово', 'ент', 'пра', 'нов', 'под', 'это', 'ски', 'раз', 'ные', 'ком', 'вер', 'ров', 'ель', 'кон', 'ско', 'тер', 'его', 'ков', 'мен', 'ден', 'ный', 'сле', 'гра', 'как', 'лен', 'дел', 'дол', 'ния', 'ным', 'сть', 'гла', 'сов', 'пер', 'ска', 'сво', 'ств', 'ить', 'спо', 'тел', 'мер', 'так', 'нии', 'дет', 'дер', 'ном', 'тов', 'сту', 'все', 'слу', 'жен', 'тор', 'общ', 'на', 'по', 'ра', 'ли', 'ни', 'ре', 'за', 'ет', 'не', 'да', 'но', 'ро', 'го', 'ст', 'та', 'ка', 'ми', 'ла', 'ко', 'ен', 'ви', 'ль', 'во', 'ов', 'об', 'ло', 'то', 'ру', 'мо', 'ма', 'ва', 'ти', 'бо', 'де', 'со', 'ин', 'ер', 'че', 'ся', 'из', 'вы', 'до', 'ки', 'па', 'ци', 'ть', 'ме', 'от', 'ри', 'ди', 'ле', 'ча', 'ит', 'те', 'ве', 'ду', 'бы', 'са', 'ел', 'га', 'ры', 'ля', 'су', 'сс', 'ку', 'се', 'ны', 'же', 'ну', 'чи', 'од', 'ей', 'му', 'сь', 'си', 'бу', 'пр', 'ши', 'ги', 'он', 'ск', 'пу', 'ор', 'ба', 'м,', 'ют', 'жа', 'зи', '\".', 'пи', 'ты', 'бе', 'пе', 'ии', 'щи', 'мы', 'ту', 'жи', 'хо', 'ря', '\",', 'ев', 'ша', 'фи', 'ня', 'би', 'лу', 'ем', 'лю', 'гу', 'ак', '00', 'ий', 'в', 'е', 'с', ',', 'и', 'д', 'м', 'я', '.', 'а', 'к', 'й', 'л', 'н', 'о', 'р', 'т', 'у', 'х', 'з', 'ч', '\"', 'ж', 'ю', 'г', 'ы', 'ш', 'б', '-', 'п', 'ф', 'ц', 'ь', 'щ', '1', 'э', 'e', '2', 'a', 'o', 's', 'r', '0', '9', 'n', 'i', 't', '5', '3', 'c', '4', 'l', 'ъ', '6', 'm', ')', '(', '8', '7', ':', 'd', 'u', 'p', 'b', 'w', 'f', 'h', 'g', 'k', 'y', 'v', 'x', '%', ';', 'z', 'j', '/', \"'\", '!', 'q', '&', '№', '?', '$', '—', '«', '»', '–', '+', '@', '“', '”', '·', 'ё', '>', '=', '_', '*', '\\xad', '•', '…', '’', '£', '<', 'ї', '|', '[', ']', '~', '#', 'і', '`']\n",
      "Количество токенов: 312\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(corpus)  # Step 1\n",
    "\n",
    "num_merges = 20 # Hyperparameter\n",
    "k = 10\n",
    "for i in range(num_merges):\n",
    "\n",
    "    pairs = get_stats(vocab)  # Step 2\n",
    "\n",
    "    if not pairs:\n",
    "        break\n",
    "\n",
    "    # step 3\n",
    "    best_top_k = sorted(pairs, key=pairs.get, reverse=True)[:k]\n",
    "    for best in best_top_k:\n",
    "        vocab = merge_vocab(best, vocab)\n",
    "\n",
    "print('Токены после BPE (отсортированные)')\n",
    "tokens_frequencies, vocab_tokenization = get_tokens_from_vocab(vocab)\n",
    "sorted_tokens_tuple = sorted(tokens_frequencies.items(), key=lambda item: (len(item[0]), item[1]), reverse=True)\n",
    "sorted_tokens = [token for (token, freq) in sorted_tokens_tuple]\n",
    "print('Токены: {}'.format(sorted_tokens))\n",
    "print('Количество токенов: {}'.format(len(tokens_frequencies.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['гу', 'б', 'ер', 'на', 'тор', 'на', 'з', 'ва', 'л', '1', '2', 'гла', 'в', 'ных', 'до', 'сти', 'ж', 'ени', 'й', 'ко', 'ст', 'ро', 'м', 'ской', 'об', 'ла', 'сти', 'в', 'это', 'м', 'го', 'ду', 'пере', 'д', 'на', 'сту', 'п', 'л', 'ение', 'м', 'ново', 'го', 'го', 'да', 'гу', 'б', 'ер', 'на', 'тор', 'с', 'ер', 'г', 'ей', 'с', 'ит', 'ни', 'ков', 'под', 'ве', 'л', 'и', 'то', 'ги', 'у', 'х', 'од', 'я', 'щ', 'его', 'го', 'да', 'и', 'на', 'з', 'ва', 'л', 'гла', 'в', 'ные', 'до', 'сти', 'ж', 'ения', 'ко', 'ст', 'ро', 'м', 'ской', 'об', 'ла', 'сти', ',', 'у', 'з', 'на', 'л', 'k', 'o', 's', 't', 'r', 'o', 'm', 'a', 't', 'o', 'd', 'a', 'y', 'с', 'дел', 'а', 'л', 'он', 'это', 'в', 'ин', 'ста', 'гра', 'ме']\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_text(string=yandex_news.lower(), sorted_tokens=sorted_tokens, unknown_token='</u>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "Мне кажется, лучший результат показывает второй эксперимент - там еще не выделяются части корней слов типа \"росси\", \"сообщ\", но уже есть частотные аффиксы типа \"тель\", \"ова\", \"ени\" и \"про\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_ok.csv')\n",
    "df['text'].to_csv('dataset_ok_text.csv', index=False)\n",
    "\n",
    "subtoken = CharBPETokenizer()\n",
    "subtoken.train('dataset_ok_text.csv', vocab_size=3000)\n",
    "\n",
    "subtoken.save('subtoken_CBPE')\n",
    "subtoken = Tokenizer.from_file('subtoken_CBPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df['text'].tolist()\n",
    "N = len(data)\n",
    "K = len(subtoken.get_vocab())\n",
    "\n",
    "X = lil_matrix((N, K))\n",
    "X_idf = lil_matrix((N, K))\n",
    "for i, doc in enumerate(data):\n",
    "    all_tokens = subtoken.encode(doc).ids\n",
    "    for token in all_tokens:\n",
    "        if X_idf[i, token] == 0:\n",
    "            X_idf[i, token] = 1\n",
    "        X[i, token] += 1\n",
    "        \n",
    "idf = pd.Series(X_idf.sum(axis=0).tolist()[0])\n",
    "idf = idf.apply(lambda x: np.log((1 + idf.shape[0]) / (1 + x)) + 1)\n",
    "X = X.multiply(lil_matrix(idf.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71987x3000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1696771 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'], test_size=0.3, stratify=df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(n_jobs=-1, verbose=1, max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.72314321, 0.7168705 , 0.72426396, 0.69513566, 0.75557527,\n",
       "       0.70169099, 0.75323991, 0.7220809 ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, df['label'], cv=kfold, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    4.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.7s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.93399267, 0.93254806, 0.93343705, 0.93376306, 0.93954212,\n",
       "       0.93187375, 0.93365192, 0.93231829])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(clf, X, df['label'], cv=kfold, scoring='f1_micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    3.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=-1, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=1,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      INSULT       0.82      0.74      0.78      2575\n",
      "      NORMAL       0.95      0.97      0.96     18317\n",
      "   OBSCENITY       0.55      0.40      0.47       205\n",
      "      THREAT       0.68      0.63      0.65       500\n",
      "\n",
      "    accuracy                           0.93     21597\n",
      "   macro avg       0.75      0.69      0.72     21597\n",
      "weighted avg       0.93      0.93      0.93     21597\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "Результат классификации по метрике F1 неплохой, расхождение в значениях macro и micro связаны с дисбалансом в классах"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
