{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Ngrams.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yudinatatiana/Comp-ling_computational_linguistics/blob/main/Ngrams.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2cE1eYRD7Zb"
      },
      "source": [
        "## Языковое моделирование"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yymsmkTCD7Zb"
      },
      "source": [
        "Языковое моделирование заключается в приписывании вероятности последовательности слов. Сейчас языковые модели используются практически во всех nlp задачах. Всякие Берты и Элмо - языковые модели. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJcIT_6lD7Zb"
      },
      "source": [
        "Это достаточно сложная тема, поэтому будем разбирать постепенно. Сегодня разберём самые основы. Научимся приписывать вероятность последовательности слов и попробуем генерировать текст."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0F2jMNCD7Zb"
      },
      "source": [
        "Возьмем два разных корпуса: новостной и сообщения с 2ch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SoMEthtHIk12",
        "outputId": "21b93b42-db7e-48db-db30-d8624175fd5e"
      },
      "source": [
        "import os\n",
        "os.listdir()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config', 'lenta.txt', '2ch_corpus.txt', 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYFUHgZ6D7Zc"
      },
      "source": [
        "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
        "dvach = open('2ch_corpus.txt').read()[:300000]\n",
        "# !!! двач не самое приятное место, большое количество текстов в этом корпусе токсичные\n",
        "\n",
        "news = open('lenta.txt').read()[:300000]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTdjuW2BD7Zc"
      },
      "source": [
        "По длине оно сопоставимы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ap92Zq4D7Zd",
        "outputId": "ca7e8659-9719-4c7c-f2e3-554f3821570c"
      },
      "source": [
        "print(\"Длина 1 -\", len(dvach))\n",
        "print(\"Длина 2 -\", len(news))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина 1 - 400000\n",
            "Длина 2 - 400000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoonMcqVD7Zf"
      },
      "source": [
        "Напишем простую функцию для нормализации. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZer48DyKaAn",
        "outputId": "c0471243-fe7d-47e7-cf67-95178ab4da72"
      },
      "source": [
        "!pip install razdel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: razdel in /usr/local/lib/python3.6/dist-packages (0.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3U3bK5W0D7Zf"
      },
      "source": [
        "from string import punctuation\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "import numpy as np\n",
        "\n",
        "def normalize(text):\n",
        "    normalized_text = [word.text.strip(punctuation) for word \\\n",
        "                                                            in razdel_tokenize(text)]\n",
        "    normalized_text = [word.lower() for word in normalized_text if word and len(word) < 20 ]\n",
        "    return normalized_text\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nT3BaBASD7Zg"
      },
      "source": [
        "Сравним тексты по токенам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AORLSbXjD7Zh"
      },
      "source": [
        "norm_dvach = normalize(dvach)\n",
        "norm_news = normalize(news)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "416xaFNuD7Zh",
        "outputId": "cf646c4d-52c0-4189-b52f-8f1c0540798f"
      },
      "source": [
        "print(\"Длина корпуса токсичных постов в токенах -\", len(norm_dvach))\n",
        "print(\"Длина корпуса новостных текстов в токенах - \", len(norm_news))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Длина корпуса токсичных постов в токенах - 63294\n",
            "Длина корпуса новостных текстов в токенах -  51538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ty2k86XD7Zh"
      },
      "source": [
        "И по уникальным токенам"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRjLfeGtD7Zh",
        "outputId": "0ff4d968-e957-4319-f8fd-27dca5fd293c"
      },
      "source": [
        "print(\"Уникальных токенов в корпусе токсичных постов -\", len(set(norm_dvach)))\n",
        "print(\"Уникальный токенов в корпусе новостных текстов - \", len(set(norm_news)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Уникальных токенов в корпусе токсичных постов - 16693\n",
            "Уникальный токенов в корпусе новостных текстов -  15345\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPWTfTq6D7Zi"
      },
      "source": [
        "Посчитаем, сколько раз встречаются слова и выведем самые частотные."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVijdXECD7Zi"
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KZgcwQMD7Zi"
      },
      "source": [
        "vocab_dvach = Counter(norm_dvach)\n",
        "vocab_news = Counter(norm_news)\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymr_mXbJD7Zi",
        "outputId": "28ac6770-549e-4d85-92c3-9d38fdb98b14"
      },
      "source": [
        "vocab_dvach.most_common(10)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 1870),\n",
              " ('не', 1638),\n",
              " ('в', 1615),\n",
              " ('на', 1041),\n",
              " ('что', 949),\n",
              " ('а', 751),\n",
              " ('я', 683),\n",
              " ('это', 643),\n",
              " ('с', 613),\n",
              " ('как', 577)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wawpCDNHD7Zi",
        "outputId": "51003b1e-e648-499f-bd32-4c6473308115"
      },
      "source": [
        "vocab_news.most_common(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('в', 2447),\n",
              " ('и', 1209),\n",
              " ('на', 883),\n",
              " ('по', 702),\n",
              " ('с', 566),\n",
              " ('что', 560),\n",
              " ('не', 422),\n",
              " ('как', 322),\n",
              " ('из', 296),\n",
              " ('о', 268)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppamqGEmD7Zi"
      },
      "source": [
        "Сравнивать употребимость конкретных слов в разных текстах в абсолютных числах неудобно. Нормализуем счётчики на размеры текстов. Так у нас получается вероятность слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UapqiZDzD7Zi",
        "outputId": "1671c568-915e-4e2c-c820-4556146c0d60"
      },
      "source": [
        "probas_dvach = Counter({word:c/len(norm_dvach) for word, c in vocab_dvach.items()})\n",
        "probas_dvach.most_common(20)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('и', 0.02954466458116093),\n",
              " ('не', 0.025879230258792303),\n",
              " ('в', 0.025515846683729894),\n",
              " ('на', 0.016447056593041996),\n",
              " ('что', 0.014993522292792365),\n",
              " ('а', 0.011865263690081208),\n",
              " ('я', 0.010790912250766265),\n",
              " ('это', 0.010158940815875123),\n",
              " ('с', 0.009684962239706765),\n",
              " ('как', 0.009116187948304736),\n",
              " ('ты', 0.008958195089581952),\n",
              " ('но', 0.006509305779378772),\n",
              " ('у', 0.006051126489082693),\n",
              " ('то', 0.005671943628148008),\n",
              " ('так', 0.005419155054191551),\n",
              " ('если', 0.004992574335640029),\n",
              " ('все', 0.004866180048661801),\n",
              " ('же', 0.0046449900464499),\n",
              " ('он', 0.004486997187727115),\n",
              " ('по', 0.004107814326792429)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5Sk47VVD7Zi",
        "outputId": "baafd17e-0e49-4389-8928-7a06ef448983"
      },
      "source": [
        "probas_news = Counter({word:c/len(norm_news) for word, c in vocab_news.items()})\n",
        "probas_news.most_common(20)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('в', 0.04747952966742986),\n",
              " ('и', 0.023458419030618186),\n",
              " ('на', 0.017132989250650005),\n",
              " ('по', 0.013621017501649268),\n",
              " ('с', 0.01098218790019015),\n",
              " ('что', 0.010865768947184601),\n",
              " ('не', 0.008188133028056968),\n",
              " ('как', 0.006247817144631146),\n",
              " ('из', 0.005743335014940432),\n",
              " ('о', 0.005200046567581202),\n",
              " ('к', 0.0041328728316970004),\n",
              " ('россии', 0.003977647561022935),\n",
              " ('за', 0.003395552795995188),\n",
              " ('также', 0.0030851022546470566),\n",
              " ('для', 0.00304629593697854),\n",
              " ('от', 0.003026892778144282),\n",
              " ('его', 0.0030074896193100237),\n",
              " ('сегодня', 0.0029880864604757656),\n",
              " ('он', 0.002891070666304474),\n",
              " ('а', 0.002755248554464667)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc8h6K1OD7Zj"
      },
      "source": [
        "Эти вероятности уже можно использовать, чтобы ответить на вопрос - это предложение больше подходит для новостей или для анонимного форума?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5CoFFibD7Zj"
      },
      "source": [
        "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала'\n",
        "\n",
        "prob = Counter({'news':0, 'dvach':0})\n",
        "\n",
        "for word in normalize(phrase):\n",
        "    prob['dvach'] += probas_dvach.get(word, 0)\n",
        "    prob['news'] += probas_news.get(word, 0)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZSwaVKUD7Zj",
        "outputId": "677bab6f-1f2f-496f-dbee-b85e981dac20"
      },
      "source": [
        "prob.most_common()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dvach', 0.03461623534616236), ('news', 0.013640420660483528)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVQCD5DQD7Zj"
      },
      "source": [
        "phrase = 'как вы смотрите эту залупу, серьезно. в чем прикол ваще это смотреть?'\n",
        "\n",
        "prob = Counter({'news':0, 'dvach':0})\n",
        "\n",
        "for word in normalize(phrase):\n",
        "    prob['dvach'] += probas_dvach.get(word, 0)\n",
        "    prob['news'] += probas_news.get(word, 0)\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oEcD72WD7Zj",
        "outputId": "435088df-9300-4a1a-c4e8-a40b51559426"
      },
      "source": [
        "prob.most_common()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('news', 0.05673483643137103), ('dvach', 0.04891458906057446)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGscGuT0D7Zk"
      },
      "source": [
        "Результаты получаются не очень точные. Возможно это из-за того, что мы считаем слова независимыми друг от друга. А это очевидно не так"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9yPOJqD7Zk"
      },
      "source": [
        "По-хорошему вероятность последовательности нужно расчитывать по формуле полной вероятности. Но у нас не очень большие тексты и мы не можем получить вероятности для длинных фраз (их просто может не быть в текстах). Поэтому мы воспользуемся предположением Маркова и будем учитывать только предыдущее слово."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE09L6TeD7Zk"
      },
      "source": [
        "Чтобы расчитать вероятность с таким предположением, нам достаточно найти количество вхождений для каждого биграмма."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "528POajbD7Zk"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def ngrammer(tokens, n=2):\n",
        "    ngrams = []\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(' '.join(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcL0i3rZK6zq",
        "outputId": "827c1725-89dd-49fd-8594-6e3ef6376c04"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8CQoKM_D7Zk"
      },
      "source": [
        "Для того, чтобы у нас получились честные вероятности и можно было посчитать вероятность первого слова, нам нужно добавить тэг маркирующий начало предложений \\< start \\>\n",
        "\n",
        "Дальше мы попробуем сгенерировать текст, используя эти вероятности, и нам нужно будет когда-то остановится. Для этого добавим тэг окончания \\< end \\>\n",
        "\n",
        "Ну и поделим все на предложения"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kYU69Z8D7Zk"
      },
      "source": [
        "sentences_dvach = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
        "sentences_news = [['<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN6-PllXD7Zk"
      },
      "source": [
        "unigrams_dvach = Counter()\n",
        "bigrams_dvach = Counter()\n",
        "\n",
        "for sentence in sentences_dvach:\n",
        "    unigrams_dvach.update(sentence)\n",
        "    bigrams_dvach.update(ngrammer(sentence))\n",
        "\n",
        "\n",
        "unigrams_news = Counter()\n",
        "bigrams_news = Counter()\n",
        "\n",
        "for sentence in sentences_news:\n",
        "    unigrams_news.update(sentence)\n",
        "    bigrams_news.update(ngrammer(sentence))\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1M9FekyD7Zk",
        "outputId": "5a27123e-a7cf-4281-da82-22fde2a72764"
      },
      "source": [
        "len(unigrams_dvach)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16695"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkfClo1jD7Zk",
        "outputId": "2d676f1a-8ce9-4b67-bf44-123c544b086d"
      },
      "source": [
        "bigrams_news.most_common(10)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<start> в', 244),\n",
              " ('<start> по', 212),\n",
              " ('<start> как', 142),\n",
              " ('риа новости', 84),\n",
              " ('<start> на', 71),\n",
              " ('по словам', 64),\n",
              " ('что в', 60),\n",
              " ('в москве', 58),\n",
              " ('об этом', 57),\n",
              " ('<start> об', 53)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xpor9KmD7Zk"
      },
      "source": [
        "Чтобы посчитать условную вероятность мы можем поделить количество вхождений на количество вхождений первого слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNv8rQgkD7Zl"
      },
      "source": [
        "phrase = 'Технические возможности устаревшего российского судна не позволили разгрузить его у терминала''Ныть надо меньше и работать больше.'\n",
        "# phrase = 'как вы смотрите эту залупу, серьезно. в чем прикол ваще это смотреть?'\n",
        "prob = Counter()\n",
        "for ngram in ngrammer(['<start>'] + normalize(phrase) + ['<end>']):\n",
        "    word1, word2 = ngram.split()\n",
        "    \n",
        "    if word1 in unigrams_dvach and ngram in bigrams_dvach:\n",
        "        prob['dvach'] += np.log(bigrams_dvach[ngram]/unigrams_dvach[word1])\n",
        "    else:\n",
        "        prob['dvach'] += np.log(0.001)\n",
        "    \n",
        "    if word1 in unigrams_news and ngram in bigrams_news:\n",
        "        prob['news'] += np.log(bigrams_news[ngram]/unigrams_news[word1])\n",
        "    else:\n",
        "        prob['news'] += np.log(0.001)\n",
        "\n",
        "prob['news'] = np.exp(prob['news'])\n",
        "prob['dvach'] = np.exp(prob['dvach'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp-R4VmTD7Zl",
        "outputId": "e11e8e8c-9389-4cca-8036-dad9a46b8d6c"
      },
      "source": [
        "prob.most_common()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('dvach', 5.941770647653036e-50), ('news', 1.000000000000004e-51)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8slteAfyD7Zl"
      },
      "source": [
        "Работает получше. Мы воспользовались небольшим хаком - для слов или биграммов, которых не было у нас в словаре, прибавляли низкую вероятность. Исправить это по-нормальному - сложно, придется подробнее разбираться с вероятностями, сглаживаниями и заменой неизвестных слов. Если интрересно - в книге Журафского про это есть."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjSuyQOFD7Zl"
      },
      "source": [
        "Проблем с неизвестными словами у нас не будет, если мы будем пытаться сгенерировать новый текст. Давайте попробуем это сделать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRWUXe6ND7Zl"
      },
      "source": [
        "matrix_dvach = np.zeros((len(unigrams_dvach), \n",
        "                   len(unigrams_dvach)))\n",
        "id2word_dvach = list(unigrams_dvach)\n",
        "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
        "\n",
        "\n",
        "for ngram in bigrams_dvach:\n",
        "    word1, word2 = ngram.split()\n",
        "    matrix_dvach[word2id_dvach[word1]][word2id_dvach[word2]] =  (bigrams_dvach[ngram]/\n",
        "                                                                     unigrams_dvach[word1])\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1GfGqA7D7Zl"
      },
      "source": [
        "# создадим матрицу вероятностей перейти из 1 слов в другое\n",
        "matrix_news = np.zeros((len(unigrams_news), \n",
        "                   len(unigrams_news)))\n",
        "\n",
        "id2word_news = list(unigrams_news)\n",
        "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
        "\n",
        "\n",
        "# вероятность расчитываем точно также\n",
        "for ngram in bigrams_news:\n",
        "    word1, word2 = ngram.split()\n",
        "    matrix_news[word2id_news[word1]][word2id_news[word2]] =  (bigrams_news[ngram]/\n",
        "                                                                     unigrams_news[word1])\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpiinK8tD7Zl"
      },
      "source": [
        "Для генерации нам понадобится функция np.random.choice , которая выбирает случайный объект из заданных. Ещё в неё можно подать вероятность каждого объекта и она будет доставать по ним (не только максимальный по вероятности)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szX84mrJD7Zl"
      },
      "source": [
        "\n",
        "def generate(matrix, id2word, word2id, n=100, start='<start>'):\n",
        "    text = []\n",
        "    current_idx = word2id[start]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
        "        text.append(id2word[chosen])\n",
        "        \n",
        "        if id2word[chosen] == '<end>':\n",
        "            chosen = word2id['<start>']\n",
        "        current_idx = chosen\n",
        "    \n",
        "    return ' '.join(text)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmtfB1lDD7Zl",
        "outputId": "3bba2fbd-a4cc-48fc-d53b-bdd29deb7f16"
      },
      "source": [
        "print(generate(matrix_dvach, id2word_dvach, word2id_dvach).replace('<end>', '\\n'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "не реализован во наличие прогрессивного веб-фрейворка адекватная замена рельсам на вопрос риторический \n",
            " меня ненавидеть но тут только тогда \n",
            " прочитал ударника в секцию ходил как только правило построения твоим словам еще эпигаиральная фаза когда уже срать \n",
            " через 5 6 всего 56 учеников это точно обозначены все треды с параши \n",
            " скоро волью новую точку соединил с определением круга общения с ношпой принять за тобой \n",
            " освятил на танцы \n",
            " чтоб глаз – это же охуенным \n",
            " потом напишу стихотворение о людях больше \n",
            " 1 \n",
            " да я сегодня у вас есть такая же в бикини в жизни и\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IyrQt9OGD7Zl",
        "outputId": "f847d117-9205-4c10-f3dd-c00006fbf8da"
      },
      "source": [
        "print(generate(matrix_news, id2word_news, word2id_news).replace('<end>', '\\n'))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "нам кажется что эта секта в изготовлении бомб \n",
            " более дорогие g 4 \n",
            " президенты также проведена операция получившая название cbs в правоохранительных органах курска корреспонденту итар-тасс согласно которому он высказал мнение что в течение двух зданий в центре им потребуется ровно год назад строев председатель облизбиркома список претендентов \n",
            " теперь куплю \n",
            " дело против россии без права и законных интересов граждан \n",
            " бой продолжался полчаса \n",
            " диггеры решили уехать в случае как мэра столицы уточнил он \n",
            " здесь уже начала расследование чтобы на помощь частных предприятий связи объяснили свой долг по оценке представителя \n",
            " по подъезду в британскую лабораторию\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEjn8JkKD7Zl"
      },
      "source": [
        "Попробуйте сделать триграммную модель на основе кода выше.\n",
        "\n",
        "Подсказки:\n",
        "    - нужно будет добавить еще один тэг <start>\n",
        "    - еще одна матрица не нужна, можно по строкам хронить биграмы, а по колонкам униграммы\n",
        "    - тексты должны быть очень похоже на нормальные (если у вас получается рандомная каша, вы что-то делаете не так)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMxgkuueWBRW"
      },
      "source": [
        "# Задача 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ3_Cf-eD7Zl"
      },
      "source": [
        "sentences_dvach = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(dvach)]\n",
        "sentences_news = [['<start>', '<start>'] + normalize(text) + ['<end>'] for text in sent_tokenize(news)]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-xpCX8WMn3m"
      },
      "source": [
        "unigrams_dvach = Counter()\n",
        "bigrams_dvach = Counter()\n",
        "threegrams_dvach = Counter()\n",
        "\n",
        "for sentence in sentences_dvach:\n",
        "    unigrams_dvach.update(sentence)\n",
        "    bigrams_dvach.update(ngrammer(sentence))\n",
        "    threegrams_dvach.update(ngrammer(sentence, n=3))\n",
        "\n",
        "\n",
        "unigrams_news = Counter()\n",
        "bigrams_news = Counter()\n",
        "threegrams_news = Counter()\n",
        "\n",
        "for sentence in sentences_news:\n",
        "    unigrams_news.update(sentence)\n",
        "    bigrams_news.update(ngrammer(sentence))\n",
        "    threegrams_news.update(ngrammer(sentence, n=3))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xItIfNvPMy34"
      },
      "source": [
        "matrix_dvach = np.zeros((len(bigrams_dvach), \n",
        "                   len(unigrams_dvach)))\n",
        "id2word_dvach = list(unigrams_dvach)\n",
        "word2id_dvach = {word:i for i, word in enumerate(id2word_dvach)}\n",
        "\n",
        "id2bigrams_dvach = list(bigrams_dvach)\n",
        "bigram2id_dvach = {bigram:i for i, bigram in enumerate(id2bigrams_dvach)}\n",
        "\n",
        "\n",
        "for ngram in threegrams_dvach:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    matrix_dvach[bigram2id_dvach[(word1 + ' ' + word2)]][word2id_dvach[word3]] =  (threegrams_dvach[ngram]/\n",
        "                                                                     bigrams_dvach[(word1 + ' ' + word2)])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVUmD5dHP0X0"
      },
      "source": [
        "# создадим матрицу вероятностей перейти из биграммы в триграмму\n",
        "matrix_news = np.zeros((len(bigrams_news), \n",
        "                   len(unigrams_news)))\n",
        "\n",
        "id2word_news = list(unigrams_news)\n",
        "word2id_news = {word:i for i, word in enumerate(id2word_news)}\n",
        "\n",
        "id2bigrams_news = list(bigrams_news)\n",
        "bigram2id_news = {bigram:i for i, bigram in enumerate(id2bigrams_news)}\n",
        "\n",
        "\n",
        "# вероятность расчитываем точно также\n",
        "for ngram in threegrams_news:\n",
        "    word1, word2, word3 = ngram.split()\n",
        "    matrix_news[bigram2id_news[(word1 + ' ' + word2)]][word2id_news[word3]] =  (threegrams_news[ngram]/\n",
        "                                                                     bigrams_news[(word1 + ' ' + word2)])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIw27w1pRj1u"
      },
      "source": [
        "def generate(matrix, id2word, word2id, id2bigram, bigram2id, n=100, start='<start> <start>'):\n",
        "    text = []\n",
        "    current_idx = bigram2id[start]\n",
        "    \n",
        "    for i in range(n):\n",
        "        \n",
        "        chosen = np.random.choice(matrix.shape[1], p=matrix[current_idx])\n",
        "        text.append(id2word[chosen])\n",
        "       \n",
        "        if id2word[chosen] == '<end>':\n",
        "            chosen = bigram2id['<start> <start>']\n",
        "        else:\n",
        "            chosen = bigram2id[(id2bigram[current_idx].split()[1] + ' ' + id2word[chosen])]\n",
        "        current_idx = chosen\n",
        "    \n",
        "    return ' '.join(text)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUx1KCtXVdeK",
        "outputId": "3f68e3e4-118c-4c16-f097-f6a7978c7d65"
      },
      "source": [
        "print(generate(matrix_news, id2word_news, word2id_news, id2bigrams_news, bigram2id_news).replace('<end>', '\\n'))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "с тем что исаков больше не возьмем его заявил он \n",
            " в ходеплебисцита 78,5 жителей восточного тимора дили планируют завтра эвакуировать большинство своих сотрудников несмотря на отсутствие у него намерении баллотироватьсяна пост мэра москвы а недовольны его работой лишь 12 жителей столицы россии предпочли бы видеть 5 опрошенных ни на что они также стали жертвами оползня \n",
            " там же находится главный претендент на президентский пост мэр буэнос-айреса фернандо де ла руа \n",
            " глава mabetex не ожидает что против него в суд с иском моральный ущерб руководитель центрального избирательного штаба блока георгием боосом и секретарем координационного совета блока депутатом госдумы олегом\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06TExYhVlj_",
        "outputId": "7c2eb24f-5a69-4759-b4cd-2258ec0a3593"
      },
      "source": [
        "print(generate(matrix_dvach, id2word_dvach, word2id_dvach, id2bigrams_dvach, bigram2id_dvach).replace('<end>', '\\n'))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "в мангаче \n",
            " за что так происходит обычно \n",
            " какое-то хуевое видео \n",
            " мимо иногда играю в хорошем разрешении со сглаживанием в старые вины \n",
            " ебать чё как в районе и все кто так делают неимоверно кайфуют от того чтобы знать что на работу возьмут хачкель писать я думаю что ты брат надо себя заставлять \n",
            " начинаю мерить темпу картина другая и по еще сотне подобных причин \n",
            " привет всем новоприбывшим \n",
            " а зачем с порнографией боретесь вы че дети мизулиной да что вы смотрели \n",
            " я думаю тоже нужно работать \n",
            " в гугле ориентация больше идёт на молодых и\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "publLSoIWFGo"
      },
      "source": [
        "# Задача 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TzGta31WGd4"
      },
      "source": [
        "import itertools\n",
        "from razdel import sentenize\n",
        "from razdel import tokenize as razdel_tokenize\n",
        "from pymorphy2 import MorphAnalyzer\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import re\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stops = set(stopwords.words('russian') + [\"это\", \"весь\"])\n",
        "morph = MorphAnalyzer()\n",
        "\n",
        "def normalize(text):\n",
        "    tokens = re.findall('[а-яёa-z0-9]+', text.lower())\n",
        "    normalized_text = [morph.parse(word)[0].normal_form for word \\\n",
        "                                                            in tokens]\n",
        "    normalized_text = [word for word in normalized_text if len(word) > 2 and word not in stops]\n",
        "    \n",
        "    return normalized_text\n",
        "\n",
        "def preprocess(text):\n",
        "    sents = sentenize(text)\n",
        "    return [normalize(sent.text) for sent in sents]\n",
        "\n",
        "def ngrammer(tokens, stops, n=2):\n",
        "    ngrams = []\n",
        "    tokens = [token for token in tokens if token not in stops]\n",
        "    for i in range(0,len(tokens)-n+1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "    return ngrams"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F99SwTgOWOQB",
        "outputId": "9847879d-023e-4e2e-fe8f-6dce20a41d68"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 12.9MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 10.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 7.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 40kB 6.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 2.9MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 5.3MB/s \n",
            "\u001b[?25hInstalling collected packages: dawg-python, pymorphy2-dicts-ru, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZDbGoSeWRpT",
        "outputId": "19c53317-3524-4ddf-ef6f-28598e87cc36"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5E2bhu0fZuCv"
      },
      "source": [
        "news = open('lenta.txt').read()"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLmv0FPFWaKw"
      },
      "source": [
        "corpus = preprocess(news)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQnZ9-AiXOKA"
      },
      "source": [
        "def scorer(worda_count, wordb_count, bigram_count, len_vocab, min_count, corpus_word_count):\n",
        "    if bigram_count >= min_count:\n",
        "        prop_a = worda_count / corpus_word_count\n",
        "        prop_b = wordb_count / corpus_word_count\n",
        "        prop_ab = bigram_count / corpus_word_count\n",
        "        try: \n",
        "            return np.log(prop_ab / (prop_a * prop_b))\n",
        "        except ValueError:\n",
        "            return -1000\n",
        "    else:\n",
        "        return -1000"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MnPeW5VWrWX",
        "outputId": "db24794d-78e7-4a50-f32b-6d429acb853e"
      },
      "source": [
        "import gensim\n",
        "\n",
        "# собираем статистики\n",
        "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=0)\n",
        "\n",
        "# преобразовывать можно и через ph, но так быстрее \n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "\n",
        "# собираем статистики по уже забиграммленному тексту\n",
        "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=0)\n",
        "p2 = gensim.models.phrases.Phraser(ph2)\n",
        "\n",
        "p2[p[corpus[333]]]"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['установить',\n",
              " 'взрыв_произойти',\n",
              " 'третий',\n",
              " 'уровнечетвертый',\n",
              " 'ярус',\n",
              " 'комплекс',\n",
              " 'зал',\n",
              " 'игровой_автомат']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oulFz5riXAiV",
        "outputId": "feefde5a-6663-4e75-96c9-a5cefc54995c"
      },
      "source": [
        "# собираем статистики\n",
        "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=4.5)\n",
        "\n",
        "# преобразовывать можно и через ph, но так быстрее \n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "\n",
        "# собираем статистики по уже забиграммленному тексту\n",
        "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=4.5)\n",
        "p2 = gensim.models.phrases.Phraser(ph2)\n",
        "\n",
        "p2[p[corpus[333]]]"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['установить',\n",
              " 'взрыв',\n",
              " 'произойти',\n",
              " 'третий',\n",
              " 'уровнечетвертый',\n",
              " 'ярус',\n",
              " 'комплекс',\n",
              " 'зал',\n",
              " 'игровой_автомат']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ29EYQfcgRa",
        "outputId": "ff5ebb6b-3df1-4e8f-a813-568a1bf016f7"
      },
      "source": [
        "# собираем статистики\n",
        "ph = gensim.models.Phrases(corpus, scoring=scorer, threshold=7.5)\n",
        "\n",
        "# преобразовывать можно и через ph, но так быстрее \n",
        "p = gensim.models.phrases.Phraser(ph)\n",
        "\n",
        "# собираем статистики по уже забиграммленному тексту\n",
        "ph2 = gensim.models.Phrases(p[corpus], scoring=scorer, threshold=7.5)\n",
        "p2 = gensim.models.phrases.Phraser(ph2)\n",
        "\n",
        "p2[p[corpus[333]]]"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['установить',\n",
              " 'взрыв',\n",
              " 'произойти',\n",
              " 'третий',\n",
              " 'уровнечетвертый',\n",
              " 'ярус',\n",
              " 'комплекс',\n",
              " 'зал',\n",
              " 'игровой',\n",
              " 'автомат']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywSTulVvdqOJ"
      },
      "source": [
        "При значении threshold = 0 попадали обе биграммы, при постепенном увеличении биграммы начинали пропадать"
      ]
    }
  ]
}